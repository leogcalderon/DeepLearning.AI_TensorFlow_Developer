{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "1. Improving Computer Vision Accuracy using Convolutions.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2gh8OmDLG",
        "colab_type": "text"
      },
      "source": [
        "# DNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUe9c9sJkIi3",
        "colab_type": "text"
      },
      "source": [
        "In the previous lessons you saw how to do fashion recognition using a Deep Neural Network (DNN) containing three layers -- the input layer (in the shape of the data), the output layer (in the shape of the desired output) and a hidden layer. You experimented with the impact of different sizes of hidden layer, number of training epochs etc on the final accuracy.\n",
        "\n",
        "For convenience, here's the entire code again. Run it and take a note of the test accuracy that is printed out at the end. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeB4ymxxkIi-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "ba97430d-6921-4014-ea79-6707251e1bd9"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images / 255.0\n",
        "test_images=test_images / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5011 - accuracy: 0.8239\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3753 - accuracy: 0.8668\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3385 - accuracy: 0.8778\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3131 - accuracy: 0.8851\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2959 - accuracy: 0.8914\n",
            "313/313 [==============================] - 0s 991us/step - loss: 0.3514 - accuracy: 0.8761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUJpccAomJ54",
        "colab_type": "text"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keMDmDyUkw3I",
        "colab_type": "text"
      },
      "source": [
        "Convolutions are perfect for computer vision, because often it's features that can get highlighted like this that distinguish one item for another, and the amount of information needed is then much less...because you'll just train on the highlighted features.\n",
        "\n",
        "That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focussed, and possibly more accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUAg7nGilXtQ",
        "colab_type": "text"
      },
      "source": [
        "1. The first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the test images. If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape.\n",
        "\n",
        "\n",
        "2. The number of convolutions you want to generate. Purely arbitrary, but good to start with something in the order of 32. The size of the Convolution, in this case a 3x3 grid.\n",
        "\n",
        "3. You'll follow the Convolution with a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convolution. By specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmPr7wU9kL-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "3e9d5dcc-a2e0-43cc-dc45-8d8cea10c027"
      },
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 81s 43ms/step - loss: 0.4381 - accuracy: 0.8425\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 81s 43ms/step - loss: 0.2958 - accuracy: 0.8908\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 80s 43ms/step - loss: 0.2485 - accuracy: 0.9071\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 80s 43ms/step - loss: 0.2138 - accuracy: 0.9202\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 79s 42ms/step - loss: 0.1881 - accuracy: 0.9290\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.2523 - accuracy: 0.9085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRhAX7wUl9qa",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing the Convolutions and Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw2jTxbPmB3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "c0caa71c-d626-444c-93ba-14818e082f80"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f, axarr = plt.subplots(3,4, figsize=(8,8))\n",
        "\n",
        "FIRST_IMAGE=23\n",
        "SECOND_IMAGE=54\n",
        "THIRD_IMAGE=26\n",
        "CONVOLUTION_NUMBER = 1\n",
        "\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='gray')\n",
        "  axarr[0,x].grid(False)\n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='gray')\n",
        "  axarr[1,x].grid(False)\n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='gray')\n",
        "  axarr[2,x].grid(False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-PzetWpnhge",
        "colab_type": "text"
      },
      "source": [
        "# EXERCISES\n",
        "\n",
        "**1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.**\n",
        "\n",
        "\n",
        "*Faster training than with 64 convolutions filters but less accuracy.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd883s6Qnqbp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "b1b3a881-9b15-4f10-84bb-b047319dcaac"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               51328     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 55,098\n",
            "Trainable params: 55,098\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.5128 - accuracy: 0.8124\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.3481 - accuracy: 0.8737\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.3029 - accuracy: 0.8893\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 29s 16ms/step - loss: 0.2751 - accuracy: 0.8992\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 29s 15ms/step - loss: 0.2546 - accuracy: 0.9054\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.2924 - accuracy: 0.8904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk4O2Zj6nr1n",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**2. Remove the final Convolution. What impact will this have on accuracy or training time?**\n",
        "\n",
        "*The accuracy went up, but the training time went down.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OmNbZCBntRv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "be53fe57-653a-44ee-afd8-810be881c670"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10816)             0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               1384576   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,386,506\n",
            "Trainable params: 1,386,506\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 0.3769 - accuracy: 0.8662\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 0.2553 - accuracy: 0.9062\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.2113 - accuracy: 0.9209\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 0.1753 - accuracy: 0.9355\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 0.1477 - accuracy: 0.9447\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2621 - accuracy: 0.9109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JWkOTrhntzI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**3.  How about adding more Convolutions? What impact do you think this will have? Experiment with it.**\n",
        "\n",
        "*Less accuracy and more training time*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT0OA-wUnvdZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "e5c5fc38-9327-4663-c8aa-bf44923de53b"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 3, 3, 32)          18464     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 1, 1, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               4224      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 61,546\n",
            "Trainable params: 61,546\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 80s 43ms/step - loss: 0.6087 - accuracy: 0.7771\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 80s 43ms/step - loss: 0.4093 - accuracy: 0.8502\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 79s 42ms/step - loss: 0.3531 - accuracy: 0.8696\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 79s 42ms/step - loss: 0.3142 - accuracy: 0.8842\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 79s 42ms/step - loss: 0.2891 - accuracy: 0.8931\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.3703 - accuracy: 0.8673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9lstt1inwQ4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**4.  Remove all Convolutions but the first. What impact do you think this will have? Experiment with it.**\n",
        "\n",
        "*More training time, almost same accuracy*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzF41Nm8nxl8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "e3b48399-5f12-454d-e7d2-1e62bfff08ab"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 43264)             0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               5537920   \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 5,539,850\n",
            "Trainable params: 5,539,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 112s 60ms/step - loss: 0.3646 - accuracy: 0.8681\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 112s 60ms/step - loss: 0.2382 - accuracy: 0.9119\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 112s 60ms/step - loss: 0.1797 - accuracy: 0.9328\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 112s 60ms/step - loss: 0.1362 - accuracy: 0.9491\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 112s 60ms/step - loss: 0.1025 - accuracy: 0.9617\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.3310 - accuracy: 0.9011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoofOHY_nyCQ",
        "colab_type": "text"
      },
      "source": [
        "5.  In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdoHgG24nzmT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "f74fced3-8600-4ee2-fc09-55ce2081cce6"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (logs.get('accuracy')>0.90):\n",
        "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5, callbacks = [callbacks])\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_15 (Conv2D)           (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 11, 11, 32)        18464     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 128)               102528    \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 122,922\n",
            "Trainable params: 122,922\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.4572 - accuracy: 0.8327\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 0.3096 - accuracy: 0.8883\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.2626 - accuracy: 0.9030\n",
            "Reached 90% accuracy so cancelling training!\n",
            "1875/1875 [==============================] - 65s 34ms/step - loss: 0.2626 - accuracy: 0.9030\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2775 - accuracy: 0.9027\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}